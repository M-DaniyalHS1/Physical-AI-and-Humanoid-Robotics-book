"use strict";(globalThis.webpackChunkai_textbook_frontend=globalThis.webpackChunkai_textbook_frontend||[]).push([[229],{633(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"vla/models","title":"VLA Models and Architectures","description":"This section covers the various models and architectures used in Vision-Language-Action (VLA) systems for Physical AI and Humanoid Robotics applications.","source":"@site/docs/vla/models.md","sourceDirName":"vla","slug":"/vla/models","permalink":"/Physical-AI-and-Humanoid-Robotics-book/ur/docs/vla/models","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/models.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"VLA Models and Architectures"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to VLA (Vision-Language-Action)","permalink":"/Physical-AI-and-Humanoid-Robotics-book/ur/docs/vla/intro"},"next":{"title":"VLA Applications in Physical AI","permalink":"/Physical-AI-and-Humanoid-Robotics-book/ur/docs/vla/applications"}}');var o=s(4848),t=s(8453);const r={sidebar_position:2,title:"VLA Models and Architectures"},a="VLA Models and Architectures",l={},d=[{value:"Foundational VLA Models",id:"foundational-vla-models",level:2},{value:"RT-1 (Robotics Transformer 1)",id:"rt-1-robotics-transformer-1",level:3},{value:"BC-Z (Behavior Cloning with Z-axis)",id:"bc-z-behavior-cloning-with-z-axis",level:3},{value:"QT-1 (Q-Transformer)",id:"qt-1-q-transformer",level:3},{value:"Modern VLA Architectures",id:"modern-vla-architectures",level:2},{value:"OpenVLA Framework",id:"openvla-framework",level:3},{value:"Diffusion Policy Networks",id:"diffusion-policy-networks",level:3},{value:"Vision Encoders for VLA",id:"vision-encoders-for-vla",level:2},{value:"CNN-based Encoders",id:"cnn-based-encoders",level:3},{value:"Vision Transformers",id:"vision-transformers",level:3},{value:"Language Encoders",id:"language-encoders",level:2},{value:"BERT-based Encoders",id:"bert-based-encoders",level:3},{value:"GPT-based Encoders",id:"gpt-based-encoders",level:3},{value:"Action Decoders",id:"action-decoders",level:2},{value:"Continuous Action Spaces",id:"continuous-action-spaces",level:3},{value:"Discrete Action Spaces",id:"discrete-action-spaces",level:3},{value:"Multimodal Fusion Techniques",id:"multimodal-fusion-techniques",level:2},{value:"Early Fusion",id:"early-fusion",level:3},{value:"Late Fusion",id:"late-fusion",level:3},{value:"Cross-Attention Fusion",id:"cross-attention-fusion",level:3},{value:"Training Considerations",id:"training-considerations",level:2},{value:"Data Requirements",id:"data-requirements",level:3},{value:"Loss Functions",id:"loss-functions",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"vla-models-and-architectures",children:"VLA Models and Architectures"})}),"\n",(0,o.jsx)(n.p,{children:"This section covers the various models and architectures used in Vision-Language-Action (VLA) systems for Physical AI and Humanoid Robotics applications."}),"\n",(0,o.jsx)(n.h2,{id:"foundational-vla-models",children:"Foundational VLA Models"}),"\n",(0,o.jsx)(n.h3,{id:"rt-1-robotics-transformer-1",children:"RT-1 (Robotics Transformer 1)"}),"\n",(0,o.jsx)(n.p,{children:"RT-1 is a transformer-based model that maps visual and language observations to robot actions."}),"\n",(0,o.jsx)(n.p,{children:"Key features:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Processes images and natural language instructions"}),"\n",(0,o.jsx)(n.li,{children:"Outputs sequences of robot actions"}),"\n",(0,o.jsx)(n.li,{children:"Trained on large-scale robot datasets"}),"\n",(0,o.jsx)(n.li,{children:"Generalizable across different tasks"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Example architecture components:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class RT1Model:\n    def __init__(self):\n        self.vision_encoder = VisionTransformer()\n        self.text_encoder = TextTransformer()\n        self.action_decoder = ActionTransformer()\n\n    def forward(self, image, instruction):\n        vision_features = self.vision_encoder(image)\n        text_features = self.text_encoder(instruction)\n        # Combine features and decode to actions\n        actions = self.action_decoder(vision_features, text_features)\n        return actions\n"})}),"\n",(0,o.jsx)(n.h3,{id:"bc-z-behavior-cloning-with-z-axis",children:"BC-Z (Behavior Cloning with Z-axis)"}),"\n",(0,o.jsx)(n.p,{children:"Extends behavioral cloning with improved representation learning for manipulation tasks."}),"\n",(0,o.jsx)(n.h3,{id:"qt-1-q-transformer",children:"QT-1 (Q-Transformer)"}),"\n",(0,o.jsx)(n.p,{children:"Uses Q-learning with transformer architecture to learn policies from vision and language inputs."}),"\n",(0,o.jsx)(n.h2,{id:"modern-vla-architectures",children:"Modern VLA Architectures"}),"\n",(0,o.jsx)(n.h3,{id:"openvla-framework",children:"OpenVLA Framework"}),"\n",(0,o.jsx)(n.p,{children:"Open-source framework for vision-language-action models:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\nimport torchvision.transforms as T\n\nclass OpenVLA:\n    def __init__(self, vision_model, language_model, policy_head):\n        self.vision_encoder = vision_model\n        self.text_encoder = language_model\n        self.policy_head = policy_head\n\n    def forward(self, image, instruction):\n        # Encode visual information\n        visual_features = self.vision_encoder(image)\n\n        # Encode language instruction\n        text_features = self.text_encoder(instruction)\n\n        # Combine modalities and predict actions\n        combined_features = torch.cat([visual_features, text_features], dim=-1)\n        actions = self.policy_head(combined_features)\n\n        return actions\n"})}),"\n",(0,o.jsx)(n.h3,{id:"diffusion-policy-networks",children:"Diffusion Policy Networks"}),"\n",(0,o.jsx)(n.p,{children:"Use diffusion models for action sequence generation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class DiffusionPolicy:\n    def __init__(self, noise_schedule, policy_network):\n        self.noise_schedule = noise_schedule\n        self.policy_network = policy_network\n\n    def sample_action(self, image, instruction, num_steps=50):\n        # Start with random noise\n        action_sequence = torch.randn((batch_size, sequence_length, action_dim))\n\n        for t in reversed(range(num_steps)):\n            # Predict noise at time step t\n            predicted_noise = self.policy_network(\n                image, instruction, action_sequence, t\n            )\n\n            # Update action sequence\n            action_sequence = self.denoise(\n                action_sequence, predicted_noise, t\n            )\n\n        return action_sequence\n"})}),"\n",(0,o.jsx)(n.h2,{id:"vision-encoders-for-vla",children:"Vision Encoders for VLA"}),"\n",(0,o.jsx)(n.h3,{id:"cnn-based-encoders",children:"CNN-based Encoders"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch.nn as nn\n\nclass VisionEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = torchvision.models.resnet50(pretrained=True)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n        self.projection = nn.Linear(2048, 512)\n\n    def forward(self, x):\n        features = self.backbone(x)\n        pooled = self.adaptive_pool(features)\n        projected = self.projection(pooled.flatten(1))\n        return projected\n"})}),"\n",(0,o.jsx)(n.h3,{id:"vision-transformers",children:"Vision Transformers"}),"\n",(0,o.jsx)(n.p,{children:"More modern approach using attention mechanisms:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from transformers import ViTModel\n\nclass ViTEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n        self.projection = nn.Linear(768, 512)\n\n    def forward(self, x):\n        outputs = self.vit(x)\n        pooled_output = outputs.pooler_output\n        projected = self.projection(pooled_output)\n        return projected\n"})}),"\n",(0,o.jsx)(n.h2,{id:"language-encoders",children:"Language Encoders"}),"\n",(0,o.jsx)(n.h3,{id:"bert-based-encoders",children:"BERT-based Encoders"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from transformers import BertModel\n\nclass LanguageEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.projection = nn.Linear(768, 512)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        projected = self.projection(pooled_output)\n        return projected\n"})}),"\n",(0,o.jsx)(n.h3,{id:"gpt-based-encoders",children:"GPT-based Encoders"}),"\n",(0,o.jsx)(n.p,{children:"For instruction understanding:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from transformers import GPT2Model\n\nclass InstructionEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.gpt = GPT2Model.from_pretrained('gpt2')\n        self.projection = nn.Linear(768, 512)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n        # Use the last token's representation\n        last_hidden_state = outputs.last_hidden_state\n        pooled_output = last_hidden_state[:, -1, :]  # Last token\n        projected = self.projection(pooled_output)\n        return projected\n"})}),"\n",(0,o.jsx)(n.h2,{id:"action-decoders",children:"Action Decoders"}),"\n",(0,o.jsx)(n.h3,{id:"continuous-action-spaces",children:"Continuous Action Spaces"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ContinuousActionDecoder(nn.Module):\n    def __init__(self, action_dim):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(1024, 1024),  # Combined visual + language features\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        )\n\n    def forward(self, combined_features):\n        return self.network(combined_features)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"discrete-action-spaces",children:"Discrete Action Spaces"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class DiscreteActionDecoder(nn.Module):\n    def __init__(self, num_actions):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_actions)\n        )\n\n    def forward(self, combined_features):\n        logits = self.network(combined_features)\n        return torch.softmax(logits, dim=-1)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"multimodal-fusion-techniques",children:"Multimodal Fusion Techniques"}),"\n",(0,o.jsx)(n.h3,{id:"early-fusion",children:"Early Fusion"}),"\n",(0,o.jsx)(n.p,{children:"Combine modalities at the input level:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class EarlyFusionModel(nn.Module):\n    def __init__(self, vision_encoder, text_encoder, action_head):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.text_encoder = text_encoder\n        self.action_head = action_head\n\n    def forward(self, image, instruction):\n        vision_features = self.vision_encoder(image)\n        text_features = self.text_encoder(instruction)\n\n        # Early fusion by concatenation\n        combined = torch.cat([vision_features, text_features], dim=-1)\n        actions = self.action_head(combined)\n\n        return actions\n"})}),"\n",(0,o.jsx)(n.h3,{id:"late-fusion",children:"Late Fusion"}),"\n",(0,o.jsx)(n.p,{children:"Combine outputs from separate encoders:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class LateFusionModel(nn.Module):\n    def __init__(self, vision_encoder, text_encoder, action_head):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.text_encoder = text_encoder\n        self.fusion_layer = nn.Linear(1024, 1024)  # For fusion\n        self.action_head = action_head\n\n    def forward(self, image, instruction):\n        vision_features = self.vision_encoder(image)\n        text_features = self.text_encoder(instruction)\n\n        # Late fusion with learned weights\n        fused_features = self.fusion_layer(\n            torch.cat([vision_features, text_features], dim=-1)\n        )\n        actions = self.action_head(fused_features)\n\n        return actions\n"})}),"\n",(0,o.jsx)(n.h3,{id:"cross-attention-fusion",children:"Cross-Attention Fusion"}),"\n",(0,o.jsx)(n.p,{children:"Use attention mechanisms to combine modalities:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class CrossAttentionFusion(nn.Module):\n    def __init__(self, feature_dim=512, num_heads=8):\n        super().__init__()\n        self.vision_proj = nn.Linear(2048, feature_dim)\n        self.text_proj = nn.Linear(768, feature_dim)\n        self.cross_attn = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=num_heads\n        )\n        self.norm = nn.LayerNorm(feature_dim)\n\n    def forward(self, vision_features, text_features):\n        # Project features to same dimension\n        vis = self.vision_proj(vision_features).unsqueeze(1)  # [batch, 1, dim]\n        txt = self.text_proj(text_features).unsqueeze(1)     # [batch, 1, dim]\n\n        # Cross-attention: text attending to vision\n        fused, _ = self.cross_attn(txt, vis, vis)\n        fused = self.norm(fused + txt)  # Residual connection\n\n        return fused.squeeze(1)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"training-considerations",children:"Training Considerations"}),"\n",(0,o.jsx)(n.h3,{id:"data-requirements",children:"Data Requirements"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Large-scale robot datasets with vision, language, and action annotations"}),"\n",(0,o.jsx)(n.li,{children:"Diverse task demonstrations"}),"\n",(0,o.jsx)(n.li,{children:"Multi-modal alignment data"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"loss-functions",children:"Loss Functions"}),"\n",(0,o.jsx)(n.p,{children:"Common approaches include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Behavioral cloning loss for imitation learning"}),"\n",(0,o.jsx)(n.li,{children:"Reinforcement learning objectives"}),"\n",(0,o.jsx)(n.li,{children:"Multimodal contrastive loss for representation learning"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Task success rate"}),"\n",(0,o.jsx)(n.li,{children:"Instruction following accuracy"}),"\n",(0,o.jsx)(n.li,{children:"Generalization to novel tasks"}),"\n",(0,o.jsx)(n.li,{children:"Human-robot interaction quality"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453(e,n,s){s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const o={},t=i.createContext(o);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);