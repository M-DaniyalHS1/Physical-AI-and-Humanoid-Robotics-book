"use strict";(globalThis.webpackChunkai_textbook_frontend=globalThis.webpackChunkai_textbook_frontend||[]).push([[96],{5214(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"nvidia-isaac/examples","title":"NVIDIA Isaac Examples","description":"This section provides practical examples of using NVIDIA Isaac for Physical AI and Humanoid Robotics applications.","source":"@site/docs/nvidia-isaac/examples.md","sourceDirName":"nvidia-isaac","slug":"/nvidia-isaac/examples","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/nvidia-isaac/examples","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/nvidia-isaac/examples.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"NVIDIA Isaac Examples"},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Setup","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/nvidia-isaac/setup"},"next":{"title":"Introduction to VLA (Vision-Language-Action)","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/vla/intro"}}');var i=s(4848),t=s(8453);const o={sidebar_position:3,title:"NVIDIA Isaac Examples"},r="NVIDIA Isaac Examples for Physical AI",l={},c=[{value:"Isaac Sim Examples",id:"isaac-sim-examples",level:2},{value:"Basic Robot Simulation",id:"basic-robot-simulation",level:3},{value:"Perception Pipeline Example",id:"perception-pipeline-example",level:3},{value:"Isaac Lab Examples",id:"isaac-lab-examples",level:2},{value:"Basic Environment Setup",id:"basic-environment-setup",level:3},{value:"Humanoid Robotics Examples",id:"humanoid-robotics-examples",level:2},{value:"Walking Controller Example",id:"walking-controller-example",level:3},{value:"Physical AI Applications",id:"physical-ai-applications",level:2},{value:"Multi-Sensor Fusion",id:"multi-sensor-fusion",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Debugging",id:"debugging",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"nvidia-isaac-examples-for-physical-ai",children:"NVIDIA Isaac Examples for Physical AI"})}),"\n",(0,i.jsx)(n.p,{children:"This section provides practical examples of using NVIDIA Isaac for Physical AI and Humanoid Robotics applications."}),"\n",(0,i.jsx)(n.h2,{id:"isaac-sim-examples",children:"Isaac Sim Examples"}),"\n",(0,i.jsx)(n.h3,{id:"basic-robot-simulation",children:"Basic Robot Simulation"}),"\n",(0,i.jsx)(n.p,{children:"Create a simple URDF robot for simulation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Python example using Isaac Sim APIs\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\n\n# Create world instance\nworld = World(stage_units_in_meters=1.0)\n\n# Add robot to stage\nget_assets_root_path()\nadd_reference_to_stage(\n    usd_path="/path/to/robot.usd",\n    prim_path="/World/Robot"\n)\n\n# Simulate\nfor i in range(1000):\n    world.step(render=True)\n    if i % 100 == 0:\n        print(f"Simulation step: {i}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"perception-pipeline-example",children:"Perception Pipeline Example"}),"\n",(0,i.jsx)(n.p,{children:"Using Isaac ROS for GPU-accelerated perception:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom isaac_ros_visual_slam_msgs.msg import IsaacROSVisualSlam\n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n\n        # Subscribe to camera topics\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/rgb/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publisher for processed data\n        self.vslam_pub = self.create_publisher(\n            IsaacROSVisualSlam,\n            '/visual_slam/output',\n            10\n        )\n\n        self.get_logger().info('Perception node initialized')\n\n    def image_callback(self, msg):\n        # Process image with GPU acceleration\n        # Using Isaac ROS image pipeline\n        self.get_logger().info('Received image')\n\n        # In practice, you would use Isaac ROS nodes\n        # which automatically leverage GPU acceleration\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-lab-examples",children:"Isaac Lab Examples"}),"\n",(0,i.jsx)(n.h3,{id:"basic-environment-setup",children:"Basic Environment Setup"}),"\n",(0,i.jsx)(n.p,{children:"Using Isaac Lab for learning tasks:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import omni\nfrom omni.isaac.kit import SimulationApp\n\n# Start simulation\nconfig = {"headless": False}\nsimulation_app = SimulationApp(config)\n\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\n\n# Create world\nworld = World(stage_units_in_meters=1.0)\n\n# Add a simple cartpole\nassets_root_path = get_assets_root_path()\nif assets_root_path is None:\n    carb.log_error("Could not use Isaac Sim assets. Ensure Isaac Sim is installed.")\n\nasset_path = assets_root_path + "/Isaac/Robots/Cartpole/cartpole.usd"\nadd_reference_to_stage(usd_path=asset_path, prim_path="/World/Cartpole")\n\nworld.reset()\n\n# Run simulation\nfor i in range(1000):\n    world.step(render=True)\n    if world.current_time_step_index == 0:\n        world.reset()\n\nsimulation_app.close()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"humanoid-robotics-examples",children:"Humanoid Robotics Examples"}),"\n",(0,i.jsx)(n.h3,{id:"walking-controller-example",children:"Walking Controller Example"}),"\n",(0,i.jsx)(n.p,{children:"Example of a simple walking controller for humanoid robot:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy import signal\n\nclass WalkingController:\n    def __init__(self, robot_description):\n        self.robot_desc = robot_description\n        self.step_frequency = 1.0  # Hz\n        self.step_length = 0.3     # meters\n        self.step_height = 0.1     # meters\n\n    def generate_trajectory(self, time):\n        \"\"\"Generate walking trajectory using sinusoidal patterns\"\"\"\n        phase = time * self.step_frequency * 2 * np.pi\n\n        # Trajectory for left foot\n        left_foot_x = np.sin(phase) * self.step_length / 2\n        left_foot_z = max(0, np.sin(phase)) * self.step_height\n\n        # Trajectory for right foot (180 degree phase shift)\n        right_foot_x = np.sin(phase + np.pi) * self.step_length / 2\n        right_foot_z = max(0, np.sin(phase + np.pi)) * self.step_height\n\n        return {\n            'left_foot': {'x': left_foot_x, 'z': left_foot_z},\n            'right_foot': {'x': right_foot_x, 'z': right_foot_z}\n        }\n\n    def balance_control(self, current_state):\n        \"\"\"Simple PD controller for balance\"\"\"\n        # Calculate desired joint angles based on balance\n        # This is a simplified example\n        desired_joints = {}\n\n        # Calculate error from desired state\n        balance_error = current_state['com_error']\n\n        # Simple PD control\n        kp = 100.0  # Proportional gain\n        kd = 10.0   # Derivative gain\n\n        control_torque = -kp * balance_error['position'] - kd * balance_error['velocity']\n\n        return control_torque\n"})}),"\n",(0,i.jsx)(n.h2,{id:"physical-ai-applications",children:"Physical AI Applications"}),"\n",(0,i.jsx)(n.h3,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Combine data from multiple sensors for Physical AI tasks:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MultiSensorFusion:\n    def __init__(self):\n        self.camera_data = None\n        self.lidar_data = None\n        self.imu_data = None\n        self.fused_state = {}\n\n    def process_sensor_data(self, camera_msg, lidar_msg, imu_msg):\n        # Process camera data (GPU accelerated)\n        vision_features = self.process_vision(camera_msg)\n\n        # Process LiDAR data\n        depth_features = self.process_lidar(lidar_msg)\n\n        # Process IMU data\n        pose_features = self.process_imu(imu_msg)\n\n        # Fuse all sensor data\n        self.fused_state = self.fuse_data(\n            vision_features,\n            depth_features,\n            pose_features\n        )\n\n        return self.fused_state\n\n    def process_vision(self, camera_msg):\n        # Use Isaac ROS image processing nodes with GPU acceleration\n        # This would typically run as a separate ROS node\n        return {"features": "vision_features"}\n\n    def fuse_data(self, vision, lidar, imu):\n        # Implement sensor fusion algorithm\n        # Could use Kalman filter, particle filter, or neural fusion\n        fused_features = {}\n        return fused_features\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use GPU-accelerated algorithms where possible"}),"\n",(0,i.jsx)(n.li,{children:"Optimize physics simulation parameters"}),"\n",(0,i.jsx)(n.li,{children:"Implement efficient data structures for sensor processing"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Always validate simulation results before deployment"}),"\n",(0,i.jsx)(n.li,{children:"Implement safety limits in control algorithms"}),"\n",(0,i.jsx)(n.li,{children:"Use simulation to test failure scenarios"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"debugging",children:"Debugging"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use RViz2 alongside Isaac Sim for visualization"}),"\n",(0,i.jsx)(n.li,{children:"Log critical parameters during simulation"}),"\n",(0,i.jsx)(n.li,{children:"Implement gradual complexity increase in tasks"}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453(e,n,s){s.d(n,{R:()=>o,x:()=>r});var a=s(6540);const i={},t=a.createContext(i);function o(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);